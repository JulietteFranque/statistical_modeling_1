\documentclass[11pt]{article}
 
\usepackage[margin=.95in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 

\begin{document}
 
\title{Homework 3}
\author{Juliette Franqueville\\
}
\maketitle

\subsection*{(1) (a) Show that $E(y | \theta) = 
\xi(\theta)= \partial \psi(\theta) / \partial \theta$}

Find the MGF of $y$:
\begin{align*}
    E(e^{ty}) &= \int e^{ty}P(y|\theta)dy\\
    &=  \int h(y)e^{ty+\theta^Ty-\psi(\theta)}dy\\
     &=  \int h(y)e^{(t+\theta)^Ty -\psi(\theta) + \psi(t+\theta) -\psi(\theta)}dy\\
     &=  e^{\psi(t+\theta) -\psi(\theta)}\int h(y)e^{(t+\theta)^Ty -\psi(t+\theta)}dy\\
     &= e^{\psi(t+\theta) -\psi(\theta)}
\end{align*}

\begin{align*}
   \frac{\partial M_y(t)}{\partial t}\rvert_{t=0} = \psi'(\theta)
\end{align*}

\subsection*{(1) (b) Show that $E(\xi(\theta)) = y_0 / \lambda + c$
and $E(\xi(\theta)|y) = (y_0 + n\bar{y})/(\lambda + n)$}

\begin{align*}
    E(\psi'(\theta)) &= \int P(\theta)\psi'(\theta)d\theta\\
    &= \int h(y_0, \lambda)\text{exp}[\theta^Ty_0-\lambda \psi(\theta)]\psi'(\theta)d\theta\\
     &= \int h(y_0, \lambda)\text{exp}[\theta^Ty_0-\lambda \psi(\theta)]\frac{1}{\lambda}[y_0 - (y_0-\lambda\psi'(\theta))]d\theta\\
     &= 1/\lambda \int y_0 h(y_0, \lambda)\text{exp}[\theta^Ty_0-\lambda \psi(\theta)] - [y_0-\lambda\psi'(\theta)]\text{exp}[\theta^Ty_0-\lambda  \psi(\theta)] d\theta\\
     &= y_0/\lambda - 1/\lambda \int  h(y_0, \lambda) [y_0-\lambda\psi'(\theta)]\text{exp}[\theta^Ty_0-\lambda  \psi(\theta)] d\theta\\
     &= y_0/\lambda - 1/\lambda \int  h(y_0, \lambda) \frac{\partial}{\partial \theta}\text{exp}[\theta^Ty_0-\lambda  \psi(\theta)] d\theta\\
      &= y_0/\lambda - 1/\lambda \frac{\partial}{\partial \theta} \int  h(y_0, \lambda) \text{exp}[\theta^Ty_0-\lambda  \psi(\theta)] d\theta\\
      &= y_0/\lambda 
\end{align*}


We've shown that for the prior $P(\theta) = h(y_0, \lambda)\text{exp}(\theta^T y_0 - \lambda \psi(\theta))$, $E(\xi(\theta)) = \frac{y_0}{\lambda} + c$. We know that the posterior for $\theta$ has form $P(\theta|y, \lambda, y_0) =  h(y_0, \lambda)\text{exp}(\theta^T(y_0 + n\bar{y}) - (\lambda + n) \psi(\theta))$. So as before, $E(\xi(\theta)|y)$ will be the ratio of the two coefficients, $\frac{y_0+n\bar{y}}{\lambda+n}$. Also, I could not find where that $c$ integration constant came from since we are integrating from $-\infty$ to $+\infty$. I spoke to Dr Sarkar about it and he was not sure either, so I did not include it.


\subsection*{(2) Show that binomial and negative binomial distributions belong to exponential families}

For the binomial distribution:

\begin{align*}
    P(y) &= {n \choose y}p^y(1-p)^{n-y}\\
    &=  {n \choose y}\text{exp}\{\text{log}[p^y(1-p)^{n-y}]\}\\
     &=  {n \choose y}\text{exp}\{y\text{log}p +(n-y)\text{log}(1-p)\}\\
     &=  {n \choose y}\text{exp}\left \{y\text{log}\frac{p}{1-p} +n\text{log}(1-p)\right \}
\end{align*}

We have 
\begin{align*}
    \theta &= \text{log}\frac{p}{1-p} \\
    \text{exp}\theta &= \frac{p}{1-p} \\
     \text{exp}\theta(1-p) &=p \\
      \text{exp}\theta(1-p) &=p \\
      \text{exp}\theta - p\text{exp}\theta &=p \\
            \text{exp}  &=p(1+\text{exp}\theta) \\
             p  &=\text{exp}\theta/(1+\text{exp}\theta) \\
               1-p  &=1-\text{exp}\theta/(1+\text{exp}\theta) \\
               &=1/(1+\text{exp}\theta) 
\end{align*}
So:

\begin{align*}
    P(y) &=  {n \choose y}\text{exp}\left \{y\text{log}\frac{p}{1-p} +n\text{log}(1-p)\right \}\\
    &=  {n \choose y}\text{exp}\left \{y\text{log}\frac{p}{1-p} -n\text{log}[1+\text{exp}\theta]\right \}
\end{align*}

So we have $h(y) = {n \choose y}$, $\theta &= \text{log}\frac{p}{1-p}$ and $\psi(\theta) = n\text{log}[1+\text{exp}\theta]$

For the negative binomial distribution:

\begin{align*}
    P(y) &= {y+r-1 \choose y}(1-p)^rp^y\\
    &= {y+r-1 \choose y}\text{exp} \{ \text{log}(1-p)^rp^y \}\\
     &= {y+r-1 \choose y}\text{exp} \{ r\text{log}(1-p) + y\text{log}p \}
\end{align*}
We have:
\begin{align*}
    \theta &= \text{log}p\\
    p &= \text{exp}\theta\\
    1-p &= 1-\text{exp}\theta
\end{align*}

So we have $h(y) = {y+r-1 \choose y}$, $\theta &= \text{log}p$ and $\psi(\theta) =  -r\text{log}(1-\text{exp}\theta)$



\subsection*{(3) Let $y \sim Bin(10,\theta)$. Also, let the observed value of $y = 3$. The prior is a mixture of Betas}

\subsection*{(a) Find the posterior}

Dropping constants, we have:

\begin{align*}
    P(\theta|) &\propto P(y|\theta)P(\theta)\\
   & \propto \theta^3(1-\theta)^7 \left[\frac{\theta^9(1-\theta)^{19}}{B(10, 20)} + \frac{\theta^{19}(1-\theta)^9}{B(20, 10)}\right]\\
    & \propto  \left[\frac{\theta^{12}(1-\theta)^{26}}{B(10, 20)} + \frac{\theta^{22}(1-\theta)^{16}}{B(20, 10)}\right]\\
    & \propto  \left[\frac{B(13, 27)}{B(13, 27)}\frac{\theta^{12}(1-\theta)^{26}}{B(10, 20)} +\frac{B(23, 17)}{B(23, 17)} \frac{\theta^{22}(1-\theta)^{16}}{B(20, 10)}\right]\\
     & \propto  \left[B(13, 27)\frac{Beta(13, 27)}{B(10, 20)} +B(23,  \frac{Beta(23, 17)}{B(20, 10)}\right]\\
     & \propto  \pi_1 Beta(13, 27) + \pi_2 Beta(23, 17)
\end{align*}

With $\pi_1 = \frac{\frac{B(13, 27)}{B(10, 20)}}{\frac{B(13, 27)}{B(10, 20)} + \frac{B(23, 17)}{B(20, 10)}}$ and  $\pi_2 = \frac{\frac{B(23, 17)}{B(20, 10)}}{\frac{B(13, 27)}{B(10, 20)} + \frac{B(23, 17)}{B(20, 10)}}$


\subsection*{(b) Plot the posterior superimposed on the prior}
\subsection*{(c) Compute a 90\% posterior credible interval for $\theta$}

To obtain the prior and posteriors, we sum the pdfs of the relevant betas. To find the 90\% posterior credible interval, we find the value of $\theta$ corresponding to the location where the area under the pdf curve for the posterior is .05 and .95. 

\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{homework_3/figures/binom.png}
    \caption{Prior / Posterior and CI}
    \label{fig:my_label}
\end{figure}

\subsection*{(4) Prove that Jeffreys’ priors satisfy the invariance principle: starting with $p(\theta) \propto |I(\theta)|^{1/2}$, show that the induced prior on $\psi = g(\theta)$, where $g$ is one-one, is
$p(\psi) \propto |I(\psi)|^{1/2}$}


\begin{align*}
    I(\psi) &= -E\left(\frac{\partial ^2\mathcal{L}(\psi)}{\partial \psi \partial \psi^T}\right)\\
    &= -E\left(\frac{\partial^2\mathcal{L}(\psi)}{\partial \theta \partial \theta^T} \left[\frac{\partial \theta}{\partial \psi}\right]^2 + \frac{\partial \mathcal{L}(\psi)}{\partial \theta} \frac{\partial ^2\psi}{\partial \theta \partial \theta^T}\right)\\
     &= -\left[\frac{\partial \theta}{\partial \psi}\right]^2E\left(\frac{\partial^2\mathcal{L}(\psi)}{\partial \theta \partial \theta^T}\right)  -\frac{\partial ^2\psi}{\partial \theta \partial \theta^T}E\left(\frac{\partial \mathcal{L}(\psi)}{\partial \theta} \right)
\end{align*}

$E\left(\frac{\partial \mathcal{L}(\psi)}{\partial \theta} \right)$ is the score and its expectation is 0:

\begin{align*}
    E\left(\frac{\partial \mathcal{L}(\psi)}{\partial \theta} \right) &= \int^{\infty}_{-\infty} f(y|\psi)  \frac{\partial \mathcal{L}(\psi)}{\partial \theta} dy\\
    &= \int^{\infty}_{-\infty} f(y|\psi)  \frac{\partial \text{log}f(y|\psi) }{\partial \theta} dy\\
    &= \int^{\infty}_{-\infty} f(y|\psi) \frac{1}{f(y|\psi) }  \frac{\partial f(y|\psi) }{\partial \theta} dy\\
     &= \frac{\partial }{\partial \theta} \int^{\infty}_{-\infty}f(y|\psi) dy \\
     & = \frac{\partial }{\partial \theta} (1) = 0
\end{align*}

So we have:

\begin{align*}
    I(\psi) &=-\left[\frac{\partial \theta}{\partial \psi}\right]^2E\left(\frac{\partial^2\mathcal{L}(\psi)}{\partial \theta \partial \theta^T}\right)  \\
    &= \left[\frac{\partial \theta}{\partial \psi}\right]^2 I(\theta)\\
\end{align*}

Then, 

\begin{align*}
    P(\psi) &\propto P(\theta) \left|\text{det}\frac{\partial \psi}{\partial \theta}\right|^{-1}\\
    &\propto \text{det}[I(\theta)]^{1/2}\left|\text{det}\frac{\partial \psi}{\partial \theta}\right|^{-1}\\
     &\propto \text{det}\left[I(\psi)\left[\frac{\partial \psi}{\partial \theta}\right]^2\right]^{1/2}\left|\text{det}\frac{\partial \psi}{\partial \theta}\right|^{-1}\\
     &\propto \text{det}[I(\psi)]^{1/2}
\end{align*}
\subsection*{(5) For the Poisson likelihood model $y_1\ldots y_n
 \sim Poisson(\lambda)$, Jeffrey’s (improper) prior was derived in class. Compute the corresponding posterior. Is it proper?}

Jeffreys' prior for the Poisson distribution is $\lambda \propto \lambda^{-1/2}$

We have:

\begin{align*}
    P(\lambda|y) &\propto P(y|\lambda)P(\lambda)\\
    &\propto \prod \lambda^y\text{exp}\{-\lambda\}\lambda^{-1/2}\\
    &\propto  \lambda^{1/2 + \sum y_i - 1} \text{exp}\{-\lambda n\}\\
    &= Ga\left(1/2 + \sum y_i, n \right)
    \end{align*}
    
Since the posterior is a Gamma distribution, it is proper (integrates to 1) as long as $n > 0$.

\subsection*{(6)  Consider the likelihood model$ y1, \ldots , y_n \sim N(\mu, \sigma)$, $\mu$ known. (a) Compute the Jeffreys’ prior for $\sigma^2$ }
Note that we already derived the Fisher information matrix for the normal distribution is the previous homework.

\begin{align*}
    P(\sigma^2) &\propto |I(\sigma^2)|^{1/2}\\
    &\propto [n/(2\sigma^4)]^{1/2}\\
    &\propto 1/\sigma^2
\end{align*}

\subsection*{(b) Compute also the corresponding posterior}

\begin{align*}
    P(\sigma^2|\mu, y) &\propto P(\mu, y|\sigma^2)P(\sigma^2)\\
    &\propto  \sigma^{-n}\text{exp}\left \{ \frac{\sum(x-\mu)^2}{\sigma^2}\right \} \frac{1}{\sigma^2}\\
    &\propto  \sigma^{-n-2}\text{exp}\left \{ \frac{\sum(x-\mu)^2}{\sigma^2}\right \} \\
     &\propto  \sigma^{2\frac{1}{2}{(-n-2)}}\text{exp}\left \{ \frac{\sum(x-\mu)^2}{\sigma^2}\right \} \\
     &= IG\left(n/2, \frac{1}{2}\sum(x-\mu)^2\right) 
\end{align*}


\subsection*{(c) Draw a random sample of size 20 from a Normal(0, 1) distribution. Using these sampled values as data points and assuming the variance to now be unknown, plot the posterior superimposed on the general shape of the prior. (d) Compute a 90\% centered quantile based credible interval for $\sigma^2$. (e) Compute also a 90\% HPD interval for $\sigma^2$.}
The plot below shows the  prior and posterior and the CIs. Note that the HPD interval was found by iteratively looking for the shortest 90\% interval.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{homework_3/figures/poisson.png}
    \caption{Prior / Posterior and CIs }
    \label{fig:my_label}
\end{figure}




\end{document}
