\documentclass[12pt]{article}
 
\usepackage[margin=.95in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, graphicx, multicol, array}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 

\begin{document}
 
\title{SDS383C Final Project Report \\ \textbf{Inferring Causal Impact Using Bayesian Structural Time-series Models}} 
\author{Brodersen, K., Gallusser, F., Koehler, J., Remy, N., Scott, S.\\Reproduced by Juliette Franqueville
}
\maketitle

\begin{abstract}
    The abstract
\end{abstract}

\section{Introduction}
\subsection{Problem Statement}
The paper I chose  presents a Bayesian methodology for inferring causal impact of market interventions on a metric of interest. A market intervention may be the launch of an advertising campaign, the release of a new product, a feature change in a product, or others. It is desirable for companies to understand whether a market intervention has a positive effect on a given metric, such as number of sales, revenue generated, number of users acquired, etc. 

The causal impact of a treatment is defined as the difference between the observed metric (i.e. sales, number of users acquired, etc) and the metric that would have been observed had no treatment been introduced. This paper deals with time series, so the causal impact is the difference between the observed time series (i.e. sales, number of users acquired as a function of time) and the time series that would have been observed with no market intervention. The main goals of this paper are therefore to 1) predict the time series that would have been observed with no market intervention - this is called the ``counterfactual'' and 2) compare this prediction with the observed time series, where a market intervention was conducted.  A key concept in this paper is that control time-series, such as time-series for other regions where no market intervention was introduced, can be used to predict the counterfactual.

\subsection{Previous Methods}
A typical approach for performing causal inference are ``difference-in-differences'' (DD) methods. DD models a usually based on a linear model between control in treatments groups. Figure \ref{dd} describes a simple example using data I simulated. A control group (for example, sales data from another region) time-series is  available. The treatment group undergoes a  market intervention event. We can model the treatment time series as a linear model of the control time series (in the simulation, I used $y_t = x_t + 500 + \varepsilon_t$, where $y_t$ and $x_t$ are the treatment and control data points at time $t$, respectively.). Using the linear relationship inferred from pre-market intervention data and the post-intervention control group data, the counterfactual treatment time-series can be predicted. The causal impact is then the difference between the observed treatment and predicted treatment time-series post market intervention. 

\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{../figures/dd.png}
    \caption{Illustration of a DD approach for inferring causal inference. }
    \label{dd}
\end{figure}

There are several limitations associated with this type of approach that this paper seeks to address. For example, DD models are generally not very flexible. Regression coefficients are usually static, which may not be a suitable assumption if they are evolving in time. The authors also wish to avoid using some of the variable selection methods that DD models use. For example, Lasso regression has been used for variable selection \cite{belloni2015program}, but this approach does not take uncertainty about which predictors to use and their coefficients, unlike Bayesian methods.

\subsection{Bayesian Structural Time-series models}
Bayesian Structural Time-series (BSTS) models are very flexible state-space models for time-series data. In fact, all ARIMA models can be specified in this manner. BSTS models can also account for dynamic regression coefficients. A BSTS model is specified as follows:

\begin{align} 
y_t &= Z_t^T\alpha_t + \varepsilon_t\\ \label{eq}
\alpha_{t+1} &= T_t \alpha_t + R_t \eta_t \label{eq1}
\end{align}
Where $\varepsilon_t \sim \mathcal{N}(0, \sigma^2)$ and 
$\eta_t \sim \mathcal{N}(0, Q_t)$. $ Q_t$ is a $p \times p$ matrix, which can be diagonal or not. The first line is the observation equation, and the second is the state equation. $y_t$ is the observed response (scalar), while $\alpha_t$ is a hidden state (scalar or vector, not observed). $Z_t$ is a $p \times n$ matrix, which can for example contain covariates for control groups or rows of 1s, depending on how the model is structured.  $T_t$ is a $p \times p$ ``transition matrix'' and $R_t$ is a $p \times p$ ``control matrix''. Both matrices are populated based on the model. Some common, simple forms of models expressed in this format are shown below. \\

\subsubsection{AR(1) Model}

An AR(1) model  can be written as a BSTS model using:

\begin{align}
    y_t &= \mu_t + \varepsilon_t \\
    \mu_{t+1} &= \phi \mu_t + \eta_t
\end{align}

Which can be re-written in the same format as equations \label{eq} and \label{eq1} by setting $\mu_t = \alpha_t$, $\mu_{t+1} = \alpha_{t+1}$, $Z_t^T = 1$, $T_t = \phi$, $R_t = 1$.


\subsubsection{Local Level}
A local level  can be written as:
\begin{align}
    y_{t} &= \mu_t + \varepsilon_t \\
    \mu_{t+1} &= \mu_t + \eta_t 
    \end{align}

Which can be re-written in the same format as equations \label{eq} and \label{eq1} by setting $\mu_t = \alpha_t$, $\mu_{t+1} = \alpha_{t+1}$, $Z_t^T = 1$, $T_t = 1$, $R_t = 1$.

\subsubsection{Linear Regression (Static Coefficients)}\label{linreg}
A linear regression can be written as:

\begin{align}
    y_{t} &= x_t\beta + \varepsilon_t\\
\end{align}

Which can be re-written in the same format as equations \label{eq} and \label{eq1} by setting $\alpha_t=1$,  $Z_t^T = x_t\beta$.

\subsubsection{Linear Regression (Dynamic Coefficients)}
A linear regression with dynamic coefficients can be written as:

\begin{align}
    y_{t} &= x_t\beta_t + \varepsilon_t\\
    \beta_{t+1} &= \beta_t + \eta_t
\end{align}

Which can be re-written in the same format as equations \label{eq} and \label{eq1} by setting $\beta_t = \alpha_t$, $\beta_{t+1}=\alpha_{t+1}$, $Z_t^T = x_t$, $T_t = 1$, $R_t = 1$.\\

Other state-space models not explicitly  listed include seasonal models, local linear trends, semi-local linear trends, etc. To allow for more flexibility, state components can be assembled by concatenating their observation and hidden state vectors $Z_t$ and $\alpha_t$ and re-arranging other matrices ($R_t, T_t$) as elements in block diagonal matrices. A concrete example showing how to assemble different models is shown in section \ref{toy}.




\section{Methods}
\subsection{Spike-and-Slab Priors}
The paper uses spike-and-slab priors for static regression coefficients (that is, coefficients in \ref{linreg}). Specifying spike-and-slab priors over regression coefficients is essentially a Bayesian method for variable selection, which involves keeping only necessary covariates in the model. Lasso is a typical frequentist methods for variable selection in regressions. For a static regression component with multiple $\beta_i$ coefficient formulated as:

\begin{align}
    y_t &=  \boldsymbol{X}_t\boldsymbol{\beta} + \varepsilon_t\\
     \varepsilon_t &\sim \mathcal{N}(0, \sigma^2)
\end{align}

A spike-and-slab prior has the form:

\begin{align}
    \beta_i \sim (1-\pi_i)\delta_0 + \pi_i\mathcal{N}(0, \tau^2\sigma^2)
\end{align}


Where $\pi_i \in [0, 1]$, and $\delta_0$ is the Dirac delta function. With $\pi_i = 0.5$, $\sigma^2 = 1$, $\tau^2 = 1$, this distribution is shown in Figure \ref{prior}



\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{../figures/spike.png}
    \caption{Spike-and-slab prior illustration.}
    \label{prior}
\end{figure}

I reproduced this aspect of the paper by creating a simulated dataset using:

\begin{align*}
    y_t& = x_{1, t}\beta_1 + x_{2, t}\beta_2 + \varepsilon_t\\
    \varepsilon_t &\sim \mathcal{N}(0, \sigma^2)
\end{align*}

With $\beta_1 = 10$,  $\beta_2 = 0.05$, and $\sigma^2 = 4$. Figure \ref{toydata} shows the dataset created. 

\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{../figures/toydata.png}
    \caption{Simulated data for reproduction of regression coefficient inference using spike-and-slab priors.}
    \label{toydata}
\end{figure}

I used the following priors:

\begin{align*}
    \tau^2 &\sim IG(1, s^2/2)\\
    \sigma^2 &\sim IG(\alpha_1, \alpha_2)\\
    \pi_i &\sim Bernoulli(\theta)\\
    \theta & \sim Beta(a, b)
\end{align*}

The conditional posteriors were (note that I checked my answers against \cite{andrade2020disjunct}):

\begin{align}
    P(\theta|\boldsymbol{\pi}) &\sim Beta(a + \sum_{i=1}^p \pi_i, b + \sum_{i=1}^p [1-\pi_i])\\
    z_i &\sim Bernoulli(\pi_i)\\
    P(\boldsymbol{\beta}|\sigma^2, \tau^2) &= \begin{cases}
     \mathcal{N}\left(\left[\boldsymbol{X}^T\boldsymbol{X} \frac{1}{\sigma^2} + \boldsymbol{I}\frac{1}{\sigma^2\tau^2}\right]^{-1}\boldsymbol{X}^T \boldsymbol{y} \frac{1}{\sigma^2}, \left[\boldsymbol{X}^T\boldsymbol{X} \frac{1}{\sigma^2} + \boldsymbol{I}\frac{1}{\sigma^2\tau^2}\right]^{-1}\right)& \text{if } z_i = 1\\
    0             & \text{if } z_i = 0
\end{cases}\\
P(\sigma^2|\boldsymbol{\beta}) &\sim IG\{\alpha_1 + n/2, \alpha_2 + \frac{1}{2}(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})^T(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta})\} \\
P(\tau^2|\boldsymbol{\beta}, \boldsymbol{\pi}) &\sim IG(1/2 + 1/2\sum_1^{p}\pi_i, s^2/2 + \boldsymbol{\beta}^T\boldsymbol{\beta}/2\sigma^2)\\
\end{align}
And finally, $\pi_i$ is updated using 

\begin{align}
    1-\pi_i = \frac{1-\theta}{(\sigma^2\tau^2)^{-1/2}\text{exp}\left(\frac{\sum_1^n x_j(y_j - X_{-i, j}\beta_{-i, j})}{2\sigma^2(\sum_1^n x_j^2 + 1/\sigma^2)}\right)  \left(\frac{\sigma^2}{\sum_1^n x_j^2 + 1/\sigma^2}\right)^{1/2}\theta   + (1-\theta)  }
\end{align}

Where $\boldsymbol{X}_{-i}$ and $\boldsymbol{\beta}_{-i}$ are $\boldsymbol{X}$ and $\boldsymbol{\beta}$ with the column and row corresponding to $i$ removed, respectively. At each iteration, $z_i$ is drawn from a Bernoulli distribution with $\pi_i$ parameter. Figure \ref{post} below show the posteriors for $\beta_1$ and $\beta_2$ from the Gibbs sampling algorithm:

\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{../figures/posteriors.png}
    \caption{Posteriors for $\beta_1$ and $\beta_2$.}
    \label{post}
\end{figure}

As expected, the posterior for $\beta_1$ does not exhibit a strong spike, but the posterior for  $\beta_2$ exhibits a strong spike at 0 (its MAP is 0). This suggests that $\beta_1$ needs to be included in the model, but $\beta_2$ does not. Formally, variable selection from those posteriors can be done using the posterior inclusion probabilities ($\pi_i$). I reproduced this aspect of the paper in isolation, but of course, this step happens in combination with sampling the other parameters in the model (e.g. local level, dynamic regression coefficients, etc). Note that there may be some differences between what I implemented and what the paper implemented (due to the fact that some details were omitted in the paper), but the general approach is the same. 

\subsection{Kalman Filtering and Smoothing}

The state vector $\alpha_t$ in BSTS models is unobserved. However, this vector must be estimated to make forecasts for the counterfactual time series. A natural way to simulate the state vector is to use Kalman filtering and smoothing. The paper used an improved algorithm from Durbin and Koopman \cite{durbin2002simple}. However, given that I was new to Kalman filtering, I only had time to get familiar/implement basic algorithms, which are described below. \\

\subsubsection{Kalman Filtering}\label{kalman}
The aim of Kalman filtering is to predict $\alpha_{t}|y_{1:t}$, that is, to predict the state vector at time $t$ given observed data up to  $t$. There are two steps in Kalman filtering: updating and predicting. The update step is:

\begin{align*}
    \alpha_{t|t-1} &= T_t \alpha_t\\
    P_{t|t-1} &= T_t P_{t-1|t-1} T_t^T + Q_t
\end{align*}

The predicting step is:
\begin{align*}
    v_t &= y_t - Z_t^T \alpha_{t|t-1}\\
    F_t &= Z_t^T P_{t|t-1} Z_t + \sigma^2\\
    K_t &= P_{t|t-1} Z_t^T F_t^{-1}\\
    \alpha_{t|t} &= \alpha_{t|t-1} + K_t v_t\\
    P_{t|t} &= (I - K_t Z_T^T)P_{t|t-1}
\end{align*}

$ \alpha_{t|t}$ is the predicted mean for the state vector, and $P_{t|t}$ is the covariance matrix. 

\subsubsection{Kalman Smoothing}
Kalman Smoothing is an algorithm that comprises of a forward pass (which is the same as Kalman filtering), and a backward pass. The backward pass computes smoothed estimated for the state vector. I used the Rauch–Tung–Striebel algorithm for Kalman smoothing. The backward pass is done using:

\begin{align*}
    \alpha_{t|n} &= \alpha{t|t} + C_t(\alpha_{t+1|n} - \alpha_{t+1|t})\\
    P_{t|n} &= P_{t|t} + C_t(P_{t+1|n} - P_{t+1|t})C_t^T
\end{align*}

Where $C_t = P_{t|t}F_{t+1}P_{t+1|t}^{-1}$. $\alpha_{t+1|n}$, $\alpha_{t+1|t}$, $P_{t+1|n}$, $P_{t+1|t}$ are obtained from the forward pass previously run. 

\subsubsection{Forecasting}
The goal of the paper is to forecast the counterfactual time series to compare it to observed data. After using the Kalman Smoothing algorithm, data is forecast using:

\begin{align*}
    \alpha^*_{t + 1}& =  T_{t} \alpha_{t|t} \\
    P^*_{t + 1}& =  T_{t} P_{t|t} T_t^T + Q_t\\
    y^*_{t + 1} &= Z_{t + 1}^T \alpha^*_{t + 1}\\
    \sigma^{2*}_{t + 1} &= Z_{t + 1}^T P^*_{t + 1}Z_{t + 1} + \sigma^2\\
\end{align*}

For the first time step, and then for $d > 1$:


\begin{align*}
    \alpha^*_{t + d}& =  T_{t} \alpha^*_{t + d -1} \\
    P^*_{t + d}& =  T_{t} P^*_{t + d - 1} T_t^T + Q_t\\
    y^*_{t + d} &= Z_{t + 1}^T \alpha^*_{t + d}\\
    \sigma^{2*}_{t + d} &= Z_{t + d}^T P^*_{t + d}Z_{t + d} + \sigma^2\\
\end{align*}


\subsection{Gibbs Sampling}
The paper uses a Gibbs sampler that comprises of the following steps:
\begin{itemize}
    \item Sample from $P(\boldsymbol{\alpha}|\boldsymbol{y}, \boldsymbol{\theta}, \boldsymbol{\beta},  \boldsymbol{\sigma^2_{\beta}})$ using the Kalman Smoother algorithm from \cite{durbin2002simple}
    \item Sample from $\boldsymbol{\theta}\sim P(\boldsymbol{\theta}|\boldsymbol{y}, \boldsymbol{\beta},  \boldsymbol{\sigma^2_{\beta}}, \boldsymbol{\alpha})$
    \item Sample from $P(\boldsymbol{\beta},\boldsymbol{\sigma^2_{\beta}}|\boldsymbol{\alpha}, \boldsymbol{\theta}, \boldsymbol{y})$
\end{itemize}

Where $\boldsymbol{\beta}$ are static regression coefficients (with spike-and-slab priors placed on them) and $\boldsymbol{\sigma^2_\beta}$ their associated variances. $\boldsymbol{\theta}$ is the vector of remaining parameters to be estimated, and $\boldsymbol{\alpha}$ is the state vector. The Gibbs sampling algorithm yields to the stationary distribution $P(\boldsymbol{\alpha}, \boldsymbol{\theta}, \boldsymbol{\beta},  \boldsymbol{\sigma^2_{\beta}}|\boldsymbol{y})$.

\section{Example with Simulated Data}\label{toy}
The paper features an demonstration of their method using simulated data. I was unable to fully replicate their method, so I only implemented the Kalman smoothing step described in section \ref{kalman}, assuming that all parameters were known. I replicated some of the other plots using the authors' R package, which is described in detail in a previous paper \cite{scott2014predicting}. I tried to match the simulated data as closely as possible, but some details in the paper were left out.

\subsection{Data Simulation}
The BSTS model at hand comprises of a dynamic regression with two covariates and a local level. The covariates could for example represent revenue in other regions that did not receive the maket intervention. The BSTS model is structured as follows:

\begin{align}
y_t &= \begin{bmatrix} \label{q1}
1 & 0 & 0\\
0 & x_{t, 1} & 0\\
0 & 0 & x_{t, 2}
\end{bmatrix} \begin{bmatrix}
\mu_{t} \\
\beta_{t, 1} \\
\beta_{t, 2} \\
\end{bmatrix} +
\varepsilon_{t} \\
\begin{bmatrix} \label{q2}
\mu_{t+1} \\
\beta_{t+1, 1} \\
\beta_{t+1, 2} \\
\end{bmatrix} &=  \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix}  \begin{bmatrix}
\mu_{t} \\
\beta_{t, 1} \\
\beta_{t, 2} \\
\end{bmatrix} + \begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix}  \begin{bmatrix}
\eta_{t, 1} \\
\eta_{t, 2}  \\
\eta_{t, 3}  \\
\end{bmatrix} 
\end{align}

Where $\mu_t$ is the local level and $\beta_{t, 1}$ and   $\beta_{t, 1}$ are the dynamic regression coefficients for the two covariates $x_{t, 1}$ and   $x_{t, 1}$, $\varepsilon_t \sim \mathcal{N}(0, 1)$ and $\boldsymbol{\eta_t} \sim \mathcal{N}(0, \boldsymbol{Q}_t)$, where $\boldsymbol{Q}_t = 0.02 \boldsymbol{I}_{3}$. In other words, the model matrices are:

\begin{align*}
    \boldsymbol{\alpha_t} &= \begin{bmatrix}
\mu_{t} \\
\beta_{t, 1} \\
\beta_{t, 2} \\
\end{bmatrix}\\
\boldsymbol{T}_t &= \boldsymbol{R}_t = \boldsymbol{I}_3\\
\boldsymbol{Z}_t^T &=\begin{bmatrix}
1 & 0 & 0\\
0 & x_{t, 1} & 0\\
0 & 0 & x_{t, 2}
\end{bmatrix} 
\end{align*}

The covariates are unspecified trigonometric functions. I chose:

\begin{align*}
    x_{t, 1} &= sin\{(2\pi/90\}  t) \\
x_{t, 2} &= sin(\{2\pi/360\}  t)\\
\end{align*}

I simulated data using equations \ref{q1} and \ref{q2}, with $\boldsymbol{\alpha}_0 = \begin{bmatrix} 10 &5 & 0
\end{bmatrix} $. After market intervention, I simulated an ``uplift'' in the metric of interest of $50\%$. Figure \ref{data} shows the simulated dataset. 


\begin{figure}[!h]
    \centering
    \includegraphics[scale=.6
    ]{../figures/data.png}
    \caption{Simulated data.}
    \label{data}
\end{figure}




\subsection{Kalman Smoothing}
I used the Kalman filtering/smoothing/forecasting steps described in section \ref{kalman} to predict the counterfactual time series. 

\subsection{Results}

\subsection{Model}



\bibliographystyle{ieeetr} 
\bibliography{ref}
\end{document}
